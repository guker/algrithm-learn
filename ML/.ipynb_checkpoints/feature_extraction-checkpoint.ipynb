{
 "metadata": {
  "name": "",
  "signature": "sha256:381cef4c76ff52d73d50f359a6ce92acc9c7f32f160c3cc2b6d3cf4b1340627f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u7279\u5f81\u63d0\u53d6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import DictVectorizer\n",
      "vec = DictVectorizer()\n",
      "\n",
      "measurements = [\n",
      "        {'city': 'Dubai', 'temperature': 33.},\n",
      "        {'city': 'London', 'temperature': 12.},\n",
      "        {'city': 'San Fransisco', 'temperature': 18.},\n",
      "]\n",
      "print vec.fit_transform(measurements).toarray()\n",
      "print vec.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  1.   0.   0.  33.]\n",
        " [  0.   1.   0.  12.]\n",
        " [  0.   0.   1.  18.]]\n",
        "['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_window = [\n",
      "    {\n",
      "        'word-2': 'the',\n",
      "         'pos-2': 'DT',\n",
      "         'word-1': 'cat',\n",
      "         'pos-1': 'NN',\n",
      "         'word+1': 'on',\n",
      "         'pos+1': 'PP',\n",
      "    }\n",
      "]\n",
      "pos_vec = vec.fit_transform(pos_window)\n",
      "print pos_vec.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.  1.  1.  1.  1.  1.]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vec.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def token_features(token, part_of_speech):\n",
      "    if token.isdigit():\n",
      "        yield \"numeric\"\n",
      "    else:\n",
      "        yield \"token={}\".format(token.lower())\n",
      "        yield \"token,pos={},{}\".format(token, part_of_speech)\n",
      "    if token[0].isupper():\n",
      "        yield \"uppercase_initial\"\n",
      "    if token.isupper():\n",
      "        yield \"all_uppercase\"\n",
      "    yield \"pos={}\".format(part_of_speech)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\n",
      "     'This is the first document.',\n",
      "     'This is the second second document.',\n",
      "     'And the third one.',\n",
      "     'Is this the first document?',\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\n",
      "print [i for i in raw_X]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'pos_tagger' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-21-8d0fb8990917>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-21-8d0fb8990917>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((tok,))\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: global name 'pos_tagger' is not defined"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature Hasher"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import FeatureHasher\n",
      "hasher = FeatureHasher(input_type='string')\n",
      "X = hasher.transform(raw_X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'pos_tagger' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-16-8c41e6639c26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeatureHasher\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mhasher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureHasher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'string'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_X, y)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0m_hashing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\_hashing.pyd\u001b[0m in \u001b[0;36msklearn.feature_extraction._hashing.transform (sklearn\\feature_extraction\\_hashing.c:1649)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_iteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"string\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0m_hashing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-14-6a29edab0b5a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((tok,))\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: global name 'pos_tagger' is not defined"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "CountVectorizer\u8fdb\u884c\u7279\u5f81\u63d0\u53d6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vec = CountVectorizer(min_df=1)\n",
      "X=vec.fit_transform(corpus)\n",
      "print X.toarray()\n",
      "print vec.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 1 1 1 0 0 1 0 1]\n",
        " [0 1 0 1 0 2 1 0 1]\n",
        " [1 0 0 0 1 0 1 1 0]\n",
        " [0 1 1 1 0 0 1 0 1]]\n",
        "[u'and', u'document', u'first', u'is', u'one', u'second', u'the', u'third', u'this']\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec.vocabulary_['document']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vec.transform(['Something completely new.']).toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 0 0 0 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2\u9636gram"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_vec = CountVectorizer(ngram_range=(1,2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
      "analyze = bigram_vec.build_analyzer()\n",
      "print analyze('Bi-grams are cool!')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'bi', u'grams', u'are', u'cool', u'bi grams', u'grams are', u'are cool']\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X2=bigram_vec.fit_transform(corpus).toarray()\n",
      "print X2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0]\n",
        " [0 0 1 0 0 1 1 0 0 2 1 1 1 0 1 0 0 0 1 1 0]\n",
        " [1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0]\n",
        " [0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1]]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_idx = bigram_vec.vocabulary_.get('is this')\n",
      "X2[:, feature_idx]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "array([0, 0, 0, 1], dtype=int64)"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_idx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "7"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "TF-IDF\u5206\u6790"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "trans = TfidfTransformer()\n",
      "trans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
        "         use_idf=True)"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = trans.fit_transform(X)\n",
      "print tfidf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 1)\t0.438776742859\n",
        "  (0, 2)\t0.541976569726\n",
        "  (0, 6)\t0.358728738248\n",
        "  (0, 3)\t0.438776742859\n",
        "  (0, 8)\t0.438776742859\n",
        "  (1, 5)\t0.853225736145\n",
        "  (1, 1)\t0.272301467523\n",
        "  (1, 6)\t0.222624292325\n",
        "  (1, 3)\t0.272301467523\n",
        "  (1, 8)\t0.272301467523\n",
        "  (2, 4)\t0.552805319991\n",
        "  (2, 7)\t0.552805319991\n",
        "  (2, 0)\t0.552805319991\n",
        "  (2, 6)\t0.28847674875\n",
        "  (3, 1)\t0.438776742859\n",
        "  (3, 2)\t0.541976569726\n",
        "  (3, 6)\t0.358728738248\n",
        "  (3, 3)\t0.438776742859\n",
        "  (3, 8)\t0.438776742859\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print trans.idf_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.91629073  1.91629073  1.22314355  1.51082562  1.51082562  1.22314355\n",
        "  1.51082562  1.91629073  1.91629073  1.91629073  1.91629073  1.91629073\n",
        "  1.          1.51082562  1.91629073  1.91629073  1.91629073  1.91629073\n",
        "  1.22314355  1.51082562  1.91629073]\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "TFIDF vectorize\u5b9e\u73b0\u4e86countvectorize\u548ctfidftransformer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vec = TfidfVectorizer(min_df=1)\n",
      "print vec.fit_transform(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 1)\t0.438776742859\n",
        "  (0, 2)\t0.541976569726\n",
        "  (0, 6)\t0.358728738248\n",
        "  (0, 3)\t0.438776742859\n",
        "  (0, 8)\t0.438776742859\n",
        "  (1, 5)\t0.853225736145\n",
        "  (1, 1)\t0.272301467523\n",
        "  (1, 6)\t0.222624292325\n",
        "  (1, 3)\t0.272301467523\n",
        "  (1, 8)\t0.272301467523\n",
        "  (2, 4)\t0.552805319991\n",
        "  (2, 7)\t0.552805319991\n",
        "  (2, 0)\t0.552805319991\n",
        "  (2, 6)\t0.28847674875\n",
        "  (3, 1)\t0.438776742859\n",
        "  (3, 2)\t0.541976569726\n",
        "  (3, 6)\t0.358728738248\n",
        "  (3, 3)\t0.438776742859\n",
        "  (3, 8)\t0.438776742859\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import chardet\n",
      "text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n",
      "text2 = b\"holdselig sind deine Ger\\xfcche\"\n",
      "text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n",
      "\n",
      "decoded = [x.decode(chardet.detect(x)['encoding']) for x in (text1, text2, text3)]\n",
      "print decoded\n",
      "\n",
      "v = CountVectorizer().fit(decoded).vocabulary_\n",
      "for term in v:\n",
      "    print v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Sei mir gegr\\u0102\\u017a\\u0102\\x9ft mein Sauerkraut', u'holdselig sind deine Ger\\xfcche', u'\\ufeffAuf Fl\\xfcgeln des Gesanges, Herzliebchen, trag ich dich fort']\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bag of words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram_vec = CountVectorizer(analyzer='char', ngram_range=(5,5), min_df=1)\n",
      "counts = ngram_vec.fit_transform(['chinese words'])\n",
      "print ngram_vec.get_feature_names()\n",
      "print counts.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u' word', u'chine', u'e wor', u'ese w', u'hines', u'inese', u'nese ', u'se wo', u'words']\n",
        "[[1 1 1 1 1 1 1 1 1]]\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u' w', u'ds', u'or', u'pr', u'rd', u's ', u'wo', u'wp']\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hashing vectorizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "hv = HashingVectorizer(n_features=10)\n",
      "print hv.transform(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 2)\t0.0\n",
        "  (0, 6)\t-0.57735026919\n",
        "  (0, 7)\t0.57735026919\n",
        "  (0, 8)\t-0.57735026919\n",
        "  (1, 2)\t0.0\n",
        "  (1, 5)\t0.816496580928\n",
        "  (1, 7)\t0.408248290464\n",
        "  (1, 8)\t-0.408248290464\n",
        "  (2, 1)\t0.5\n",
        "  (2, 4)\t-0.5\n",
        "  (2, 5)\t-0.5\n",
        "  (2, 8)\t-0.5\n",
        "  (3, 2)\t0.0\n",
        "  (3, 6)\t-0.57735026919\n",
        "  (3, 7)\t0.57735026919\n",
        "  (3, 8)\t-0.57735026919\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "NLTK\u81ea\u5b9a\u4e49word tokenize"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import word_tokenize\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named nltk",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-79-ccf609a412f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mImportError\u001b[0m: No module named nltk"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}