{
 "metadata": {
  "name": "",
  "signature": "sha256:fc708889d7a9d0d5a9e59f5a7ca80276aad3df126ecc19d0c0c95179db96be0d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u7279\u5f81\u63d0\u53d6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import DictVectorizer\n",
      "vec = DictVectorizer()\n",
      "\n",
      "measurements = [\n",
      "        {'city': 'Dubai', 'temperature': 33.},\n",
      "        {'city': 'London', 'temperature': 12.},\n",
      "        {'city': 'San Fransisco', 'temperature': 18.},\n",
      "]\n",
      "print vec.fit_transform(measurements).toarray()\n",
      "print vec.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  1.   0.   0.  33.]\n",
        " [  0.   1.   0.  12.]\n",
        " [  0.   0.   1.  18.]]\n",
        "['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_window = [\n",
      "    {\n",
      "        'word-2': 'the',\n",
      "         'pos-2': 'DT',\n",
      "         'word-1': 'cat',\n",
      "         'pos-1': 'NN',\n",
      "         'word+1': 'on',\n",
      "         'pos+1': 'PP',\n",
      "    }\n",
      "]\n",
      "pos_vec = vec.fit_transform(pos_window)\n",
      "print pos_vec.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.  1.  1.  1.  1.  1.]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vec.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def token_features(token, part_of_speech):\n",
      "    if token.isdigit():\n",
      "        yield \"numeric\"\n",
      "    else:\n",
      "        yield \"token={}\".format(token.lower())\n",
      "        yield \"token,pos={},{}\".format(token, part_of_speech)\n",
      "    if token[0].isupper():\n",
      "        yield \"uppercase_initial\"\n",
      "    if token.isupper():\n",
      "        yield \"all_uppercase\"\n",
      "    yield \"pos={}\".format(part_of_speech)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [\n",
      "     'This is the first document.',\n",
      "     'This is the second second document.',\n",
      "     'And the third one.',\n",
      "     'Is this the first document?',\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\n",
      "print [i for i in raw_X]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'pos_tagger' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-21-8d0fb8990917>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-21-8d0fb8990917>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((tok,))\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: global name 'pos_tagger' is not defined"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature Hasher"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import FeatureHasher\n",
      "hasher = FeatureHasher(input_type='string')\n",
      "X = hasher.transform(raw_X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'pos_tagger' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-16-8c41e6639c26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFeatureHasher\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mhasher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureHasher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'string'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_X, y)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0m_hashing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\_hashing.pyd\u001b[0m in \u001b[0;36msklearn.feature_extraction._hashing.transform (sklearn\\feature_extraction\\_hashing.c:1649)\u001b[1;34m()\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\hashing.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_iteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"string\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0m_hashing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-14-6a29edab0b5a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((tok,))\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: global name 'pos_tagger' is not defined"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "CountVectorizer\u8fdb\u884c\u7279\u5f81\u63d0\u53d6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vec = CountVectorizer(min_df=1)\n",
      "X=vec.fit_transform(corpus)\n",
      "print X.toarray()\n",
      "print vec.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 1 1 1 0 0 1 0 1]\n",
        " [0 1 0 1 0 2 1 0 1]\n",
        " [1 0 0 0 1 0 1 1 0]\n",
        " [0 1 1 1 0 0 1 0 1]]\n",
        "[u'and', u'document', u'first', u'is', u'one', u'second', u'the', u'third', u'this']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec.vocabulary_['document']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vec.transform(['Something completely new.']).toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 0 0 0 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2\u9636gram"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigram_vec = CountVectorizer(ngram_range=(1,2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
      "analyze = bigram_vec.build_analyzer()\n",
      "print analyze('Bi-grams are cool!')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'bi', u'grams', u'are', u'cool', u'bi grams', u'grams are', u'are cool']\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X2=bigram_vec.fit_transform(corpus).toarray()\n",
      "print X2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0]\n",
        " [0 0 1 0 0 1 1 0 0 2 1 1 1 0 1 0 0 0 1 1 0]\n",
        " [1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0]\n",
        " [0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1]]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_idx = bigram_vec.vocabulary_.get('is this')\n",
      "X2[:, feature_idx]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "array([0, 0, 0, 1], dtype=int64)"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_idx"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "7"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "TF-IDF\u5206\u6790"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "trans = TfidfTransformer()\n",
      "trans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
        "         use_idf=True)"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = trans.fit_transform(X)\n",
      "print tfidf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 1)\t0.438776742859\n",
        "  (0, 2)\t0.541976569726\n",
        "  (0, 6)\t0.358728738248\n",
        "  (0, 3)\t0.438776742859\n",
        "  (0, 8)\t0.438776742859\n",
        "  (1, 5)\t0.853225736145\n",
        "  (1, 1)\t0.272301467523\n",
        "  (1, 6)\t0.222624292325\n",
        "  (1, 3)\t0.272301467523\n",
        "  (1, 8)\t0.272301467523\n",
        "  (2, 4)\t0.552805319991\n",
        "  (2, 7)\t0.552805319991\n",
        "  (2, 0)\t0.552805319991\n",
        "  (2, 6)\t0.28847674875\n",
        "  (3, 1)\t0.438776742859\n",
        "  (3, 2)\t0.541976569726\n",
        "  (3, 6)\t0.358728738248\n",
        "  (3, 3)\t0.438776742859\n",
        "  (3, 8)\t0.438776742859\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print trans.idf_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.91629073  1.91629073  1.22314355  1.51082562  1.51082562  1.22314355\n",
        "  1.51082562  1.91629073  1.91629073  1.91629073  1.91629073  1.91629073\n",
        "  1.          1.51082562  1.91629073  1.91629073  1.91629073  1.91629073\n",
        "  1.22314355  1.51082562  1.91629073]\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "TFIDF vectorize\u5b9e\u73b0\u4e86countvectorize\u548ctfidftransformer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vec = TfidfVectorizer(min_df=1)\n",
      "print vec.fit_transform(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 1)\t0.438776742859\n",
        "  (0, 2)\t0.541976569726\n",
        "  (0, 6)\t0.358728738248\n",
        "  (0, 3)\t0.438776742859\n",
        "  (0, 8)\t0.438776742859\n",
        "  (1, 5)\t0.853225736145\n",
        "  (1, 1)\t0.272301467523\n",
        "  (1, 6)\t0.222624292325\n",
        "  (1, 3)\t0.272301467523\n",
        "  (1, 8)\t0.272301467523\n",
        "  (2, 4)\t0.552805319991\n",
        "  (2, 7)\t0.552805319991\n",
        "  (2, 0)\t0.552805319991\n",
        "  (2, 6)\t0.28847674875\n",
        "  (3, 1)\t0.438776742859\n",
        "  (3, 2)\t0.541976569726\n",
        "  (3, 6)\t0.358728738248\n",
        "  (3, 3)\t0.438776742859\n",
        "  (3, 8)\t0.438776742859\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import chardet\n",
      "text1 = b\"Sei mir gegr\\xc3\\xbc\\xc3\\x9ft mein Sauerkraut\"\n",
      "text2 = b\"holdselig sind deine Ger\\xfcche\"\n",
      "text3 = b\"\\xff\\xfeA\\x00u\\x00f\\x00 \\x00F\\x00l\\x00\\xfc\\x00g\\x00e\\x00l\\x00n\\x00 \\x00d\\x00e\\x00s\\x00 \\x00G\\x00e\\x00s\\x00a\\x00n\\x00g\\x00e\\x00s\\x00,\\x00 \\x00H\\x00e\\x00r\\x00z\\x00l\\x00i\\x00e\\x00b\\x00c\\x00h\\x00e\\x00n\\x00,\\x00 \\x00t\\x00r\\x00a\\x00g\\x00 \\x00i\\x00c\\x00h\\x00 \\x00d\\x00i\\x00c\\x00h\\x00 \\x00f\\x00o\\x00r\\x00t\\x00\"\n",
      "\n",
      "decoded = [x.decode(chardet.detect(x)['encoding']) for x in (text1, text2, text3)]\n",
      "print decoded\n",
      "\n",
      "v = CountVectorizer().fit(decoded).vocabulary_\n",
      "for term in v:\n",
      "    print v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Sei mir gegr\\u0102\\u017a\\u0102\\x9ft mein Sauerkraut', u'holdselig sind deine Ger\\xfcche', u'\\ufeffAuf Fl\\xfcgeln des Gesanges, Herzliebchen, trag ich dich fort']\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n",
        "{u'mein': 12, u'deine': 1, u'herzliebchen': 9, u'gesanges': 8, u'ger\\xfcche': 7, u'sind': 16, u'fl\\xfcgeln': 4, u'gegr\\u0103\\u017a\\u0103': 6, u'sei': 15, u'fort': 5, u'trag': 17, u'ich': 11, u'sauerkraut': 14, u'auf': 0, u'dich': 3, u'des': 2, u'holdselig': 10, u'mir': 13}\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bag of words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngram_vec = CountVectorizer(analyzer='char', ngram_range=(5,5), min_df=1)\n",
      "counts = ngram_vec.fit_transform(['chinese words'])\n",
      "print ngram_vec.get_feature_names()\n",
      "print counts.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u' word', u'chine', u'e wor', u'ese w', u'hines', u'inese', u'nese ', u'se wo', u'words']\n",
        "[[1 1 1 1 1 1 1 1 1]]\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u' w', u'ds', u'or', u'pr', u'rd', u's ', u'wo', u'wp']\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hashing vectorizer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "hv = HashingVectorizer(n_features=10)\n",
      "print hv.transform(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 2)\t0.0\n",
        "  (0, 6)\t-0.57735026919\n",
        "  (0, 7)\t0.57735026919\n",
        "  (0, 8)\t-0.57735026919\n",
        "  (1, 2)\t0.0\n",
        "  (1, 5)\t0.816496580928\n",
        "  (1, 7)\t0.408248290464\n",
        "  (1, 8)\t-0.408248290464\n",
        "  (2, 1)\t0.5\n",
        "  (2, 4)\t-0.5\n",
        "  (2, 5)\t-0.5\n",
        "  (2, 8)\t-0.5\n",
        "  (3, 2)\t0.0\n",
        "  (3, 6)\t-0.57735026919\n",
        "  (3, 7)\t0.57735026919\n",
        "  (3, 8)\t-0.57735026919\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "NLTK\u81ea\u5b9a\u4e49word tokenize"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import word_tokenize\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "class LemmaTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
      "    \n",
      "vect = CountVectorizer(tokenizer = LemmaTokenizer())\n",
      "print vect.fit_transform(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\tracholar/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\Program Files\\\\Python27\\\\nltk_data'\n    - 'D:\\\\Program Files\\\\Python27\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tracholar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-9-15be8192b450>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLemmaTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 786\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 235\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-9-15be8192b450>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwnl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwnl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLemmaTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\nltk-3.0.0-py2.7.egg\\nltk\\tokenize\\__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     91\u001b[0m     along with :class:`.PunktSentenceTokenizer`).\n\u001b[0;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m     return [token for sent in sent_tokenize(text)\n\u001b[0m\u001b[0;32m     94\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\nltk-3.0.0-py2.7.egg\\nltk\\tokenize\\__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mcurrently\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[1;33m`\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\nltk-3.0.0-py2.7.egg\\nltk\\data.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\nltk-3.0.0-py2.7.egg\\nltk\\data.pyc\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mD:\\Program Files\\Python27\\lib\\site-packages\\nltk-3.0.0-py2.7.egg\\nltk\\data.pyc\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\tracholar/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\Program Files\\\\Python27\\\\nltk_data'\n    - 'D:\\\\Program Files\\\\Python27\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\tracholar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "\u56fe\u50cf\u7279\u5f81\u63d0\u53d6"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.feature_extraction import image\n",
      "\n",
      "\n",
      "one_image = np.arange(4 * 4 * 3).reshape((4,4,3))\n",
      "print one_image[:,:,0]\n",
      "\n",
      "patches = image.extract_patches_2d(one_image,(2,2), max_patches=2, random_state=0)\n",
      "print patches.shape\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0  3  6  9]\n",
        " [12 15 18 21]\n",
        " [24 27 30 33]\n",
        " [36 39 42 45]]\n",
        "(2, 2, 2, 3)\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patches = image.extract_patches_2d(one_image,(2,2))\n",
      "print patches.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(9, 2, 2, 3)\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recon = image.reconstruct_from_patches_2d(patches,(4,4,3))\n",
      "np.testing.assert_array_equal(one_image, recon)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}